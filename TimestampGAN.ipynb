{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873374ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Reshape, LSTM, LeakyReLU, GaussianNoise, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from tqdm import tqdm as tqdm\n",
    "from itertools import permutations \n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "class ConfortiGAN:\n",
    "    \n",
    "    \n",
    "    def __init__(self, path_to_log, id_column, timestamp_column, activity_column, output_path):\n",
    "        self.preprocessor = Preprocessor(path_to_log, id_column, timestamp_column, activity_column)\n",
    "        self.output_path = output_path\n",
    "        self.group = 0\n",
    "        self.latent_dim = 100\n",
    "        \n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.preprocessor.prepare_data()\n",
    "        self.n_classes = self.preprocessor.encoder.categories_[0].shape[0]\n",
    "        self.df = self.preprocessor.df\n",
    "        self.x_train = self.preprocessor.x_train\n",
    "        self.y_train = self.preprocessor.y_train\n",
    "        self.x_val = self.preprocessor.x_val\n",
    "        self.y_val = self.preprocessor.y_val\n",
    "        self.x_test = self.preprocessor.x_test\n",
    "        self.y_test = self.preprocessor.y_test\n",
    "        \n",
    "        \n",
    "    def reorder_events(self):\n",
    "        indices = self.df[self.df.equivalency_flag].activity.reset_index()['index'].to_list()\n",
    "        old_order = self.df[self.df.equivalency_flag].activity.to_list()\n",
    "        indices = self.df[self.df.equivalency_flag].activity.reset_index()['index'].to_list()\n",
    "        \n",
    "        best_order = self.get_best_order()\n",
    "    \n",
    "        best_order = best_order.best_order.to_list()\n",
    "        flat_list = [item for sublist in best_order for item in sublist]\n",
    "        best_order = flat_list\n",
    "        \n",
    "        old_order_indexed = []\n",
    "        index_dict = {key : 0 for key in set(old_order)}\n",
    "        for el in old_order:\n",
    "            old_order_indexed.append(el + str(index_dict[el]))\n",
    "            index_dict[el] += 1\n",
    "\n",
    "        best_order_indexed = []\n",
    "        index_dict = {key : 0 for key in set(best_order)}\n",
    "        for el in best_order:\n",
    "            best_order_indexed.append(el + str(index_dict[el]))\n",
    "            index_dict[el] += 1\n",
    "\n",
    "        res = sorted(old_order_indexed, key = best_order_indexed.index)\n",
    "        sorting = [old_order_indexed.index(x) for x in res]\n",
    "        new_order = [indices[i] for i in sorting]\n",
    "        for i in range(self.df.shape[0]):\n",
    "            if i in new_order:\n",
    "                continue\n",
    "            else:\n",
    "                new_order.insert(i, i)\n",
    "        \n",
    "        self.reordered_log = self.df.iloc[new_order].sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    def get_best_order(self):\n",
    "        \"\"\"\n",
    "        Calculates the best order for affected events.\n",
    "\n",
    "        Returns:\n",
    "            best_order (pandas.DataFrame): The best orders for timestamp-equivalent sequence in form of a list\n",
    "        \"\"\"\n",
    "        self.pairwise_confidences = self.get_pairwise_confidences()\n",
    "        to_repair = self.df.groupby(['id'])['activity'].apply(list).reset_index()\n",
    "        best_sequences = []\n",
    "\n",
    "        for i in tqdm(range(to_repair.shape[0])):\n",
    "            sequence = to_repair.iloc[i, -1]\n",
    "            if len(sequence) > 9: # otherwise permutations will take too long to be calculated\n",
    "                best_sequences.append(sequence)\n",
    "                continue\n",
    "            perms = list(permutations(sequence))\n",
    "            sequence_confidences = []\n",
    "\n",
    "            for perm in perms:\n",
    "                seq = [(perm[i], perm[i+1]) for i in range(len(perm) - 1)]\n",
    "                try:\n",
    "                    seq_conf = sum([pairwise_confidences[x] for x in seq])\n",
    "                except:\n",
    "                    seq_conf = 0\n",
    "                sequence_confidences.append(seq_conf)\n",
    "\n",
    "            best_sequence = perms[sequence_confidences.index(max(sequence_confidences))]\n",
    "            best_sequences.append(list(best_sequence))\n",
    "\n",
    "        to_repair['best_order'] = best_sequences\n",
    "        return to_repair\n",
    "    \n",
    "    \n",
    "    def get_pairwise_confidences(self):\n",
    "        \"\"\"\n",
    "        Calculates the pairwise confidences for all activity combinations in the event log.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "        Returns:\n",
    "            pairwise_confidences (dict): Mapping of activity combination to confidence\n",
    "        \"\"\"\n",
    "        print(\"Calculating pairwise confidences...\")\n",
    "        correct_events = self.df[~self.df['equivalency_flag']]\n",
    "        correct_events = correct_events.reset_index(drop=True)\n",
    "        occurences = {}\n",
    "        last_in_case = correct_events.reset_index().groupby('id').last()['index'].to_list()\n",
    "\n",
    "\n",
    "        for activity in tqdm(correct_events.activity.unique()):\n",
    "            subs = list(correct_events[correct_events['activity'] == activity].index + 1)\n",
    "            subs = [x for x in subs if x not in last_in_case and x < correct_events.shape[0]]\n",
    "            occurences[activity] = correct_events.iloc[subs,1].value_counts().to_dict()\n",
    "\n",
    "        pairwise_confidences = {}\n",
    "\n",
    "        for activity in correct_events.activity.unique():\n",
    "            for other_activity in correct_events.activity.unique():\n",
    "                if other_activity not in occurences[activity].keys():\n",
    "                    pairwise_confidences[(activity, other_activity)] = 0\n",
    "                else:\n",
    "                    conf = occurences[activity][other_activity] / correct_events[correct_events['activity'] == activity].shape[0]\n",
    "                    pairwise_confidences[(activity, other_activity)] = conf\n",
    "        return pairwise_confidences\n",
    "    \n",
    "    \n",
    "    def construct_networks(self):\n",
    "        print(\"Constructing networks...\")\n",
    "        self.generator = self.define_generator(n_classes = self.n_classes, latent_dim = self.latent_dim)\n",
    "        self.discriminator = self.define_discriminator(n_classes = self.n_classes)\n",
    "        self.gan = self.define_gan()\n",
    "        \n",
    "\n",
    "    def define_generator(self, n_classes, latent_dim):\n",
    "        input_sequences = Input(shape = (None, n_classes), name='input_sequences')\n",
    "        input_max_duration = Input(shape = (None, 1), name='input_max_duration')\n",
    "        input_latent = Input(shape=(None, latent_dim), name='input_latent')\n",
    "\n",
    "        layer_input = Concatenate()([input_sequences, input_max_duration, input_latent])\n",
    "        LSTM_1 = LSTM(1000, return_sequences=True, dropout=0.25)(layer_input)\n",
    "        LSTM_2 = LSTM(500, return_sequences=True, dropout=0.25)(LSTM_1)\n",
    "        LSTM_3 = LSTM(100, return_sequences=True)(LSTM_2)\n",
    "        layer_output_sequences = TimeDistributed(Dense(n_classes, activation='softmax'))(LSTM_3)\n",
    "\n",
    "        #LSTM_4 = LSTM(n_classes, return_sequences=True)(LSTM_3)\n",
    "        layer_output_durations = LSTM(1, return_sequences=True, activation = 'sigmoid')(LSTM_3)\n",
    "\n",
    "        generator = Model(inputs=[input_sequences, input_max_duration, input_latent], outputs=[layer_output_sequences, layer_output_durations])\n",
    "        return generator\n",
    "\n",
    "    \n",
    "    def define_discriminator(self, n_classes):\n",
    "        input_sequ = Input(shape = (None, n_classes), name='input_sequ')\n",
    "        input_dur = Input(shape = (None, 1), name='input_dur')\n",
    "\n",
    "        layer_input = Concatenate()([input_sequ, input_dur])\n",
    "        noise = GaussianNoise(0.2)(layer_input)\n",
    "        LSTM_1 = LSTM(1000, return_sequences=True)(noise)\n",
    "        LSTM_3 = LSTM(500, return_sequences=False)(LSTM_1)\n",
    "        dense_1 = Dense(100)(LSTM_3)\n",
    "        dense_1_act = LeakyReLU(alpha=0.2)(dense_1)\n",
    "        dense_2 = Dense(50)(dense_1_act)\n",
    "        dense_2_act = LeakyReLU(alpha=0.2)(dense_2)\n",
    "        output_classification = Dense(1, activation='sigmoid')(dense_2_act)\n",
    "\n",
    "        discriminator = Model(inputs=[input_sequ, input_dur], outputs=output_classification)\n",
    "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "        discriminator.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return discriminator\n",
    "\n",
    "    \n",
    "    def define_gan(self):\n",
    "        self.discriminator.trainable = False\n",
    "        gan_input = self.generator.input\n",
    "        generator_output = self.generator.output\n",
    "        gan_output = self.discriminator(generator_output)\n",
    "        gan = Model(gan_input, gan_output)\n",
    "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "        gan.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "        return gan\n",
    "    \n",
    "    def get_real_samples(self, n_samples, data):\n",
    "        activities = data[0]\n",
    "        durations = data[1]\n",
    "        indices = np.random.randint(0, activities.shape[0], n_samples)\n",
    "        random_samples = [activities[indices], durations[indices]]\n",
    "        y = np.ones((n_samples, 1)) #label as real data\n",
    "        return random_samples, y\n",
    "\n",
    "    def get_generator_input(self, data, n_samples, timesteps):\n",
    "        activities = data[0]\n",
    "        durations = data[1]\n",
    "\n",
    "        indices = np.random.randint(0, activities.shape[0], n_samples)\n",
    "\n",
    "        latent = np.random.randn(n_samples, timesteps, self.latent_dim)\n",
    "\n",
    "        return [activities[indices], durations[indices], latent]\n",
    "\n",
    "    def generate_fake_samples(self, data, n_samples, timesteps, latent_dim):\n",
    "        input_generator = get_generator_input(data, n_samples, timesteps, latent_dim)\n",
    "        generated = generator.predict(input_generator)\n",
    "        y = np.zeros((n_samples,1))\n",
    "        return generated, y\n",
    "\n",
    "    def train_gan(self):\n",
    "        for key in self.x_train.keys():\n",
    "            print(\"Currently at timesteps =\", key)\n",
    "            train(n_epochs=10, n_batch=256, timesteps=key)\n",
    "    \n",
    "    def train(self, n_epochs, n_batch, timesteps):\n",
    "\n",
    "        print(\"Training the networks...\")\n",
    "        bat_per_epo = int(self.x_train[timesteps][0].shape[0] / n_batch)\n",
    "        half_batch = int(n_batch / 2)\n",
    "\n",
    "        d_loss_real = []\n",
    "        d_loss_fake = []\n",
    "        g_loss = []\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "            for j in range(bat_per_epo):\n",
    "\n",
    "                # get randomly selected 'real' samples\n",
    "                disc_input_real, y_real = self.get_real_samples(half_batch, self.y_train[timesteps])\n",
    "                d_loss1, _ = self.discriminator.train_on_batch(disc_input_real, y_real)\n",
    "                d_loss_real.append(d_loss1)\n",
    "\n",
    "\n",
    "                # get randomly selected 'fake' samples\n",
    "                disc_input_fake, y_fake = self.generate_fake_samples(self.x_train[timesteps], half_batch, timesteps, 100)\n",
    "                d_loss2, _ = self.discriminator.train_on_batch(disc_input_fake, y_fake)\n",
    "                d_loss_fake.append(d_loss2)\n",
    "\n",
    "\n",
    "                x_gan = self.get_generator_input(self.x_train[timesteps], n_batch, timesteps)\n",
    "                y_gan = np.ones((n_batch, 1))\n",
    "                gan_loss = self.gan.train_on_batch(x_gan, y_gan)\n",
    "                g_loss.append(gan_loss)\n",
    "\n",
    "                print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %(i+1, j+1, bat_per_epo, d_loss1, d_loss2, gan_loss))\n",
    "\n",
    "                file = open(self.output_path + '/logging/metric_log.txt', 'a')\n",
    "                file.write('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %(i+1, j+1, bat_per_epo, d_loss1, d_loss2, gan_loss))\n",
    "                file.write('\\n')\n",
    "                file.close()\n",
    "\n",
    "                if j % 250 == 0:    \n",
    "                    self.generator.save(self.output_path + '/model_files/epoch_' + str(i).zfill(4) + '_batch_' + str(j).zfill(4) + '.h5')\n",
    "                    self.evaluate_validation_set(timesteps)\n",
    "\n",
    "\n",
    "    def three_d_categorical_accuracy(self, y_true, y_pred):\n",
    "        # max values along the second axis\n",
    "        indices_true = np.argmax(y_true[0], axis=2)\n",
    "        indices_pred = np.argmax(y_pred[0], axis=2)\n",
    "\n",
    "        # Check for equality along individual elements\n",
    "        equality = indices_true == indices_pred\n",
    "        hits = [x.all() for x in equality]\n",
    "\n",
    "        cat_acc = sum(hits) / y_true[0].shape[0]\n",
    "        return cat_acc\n",
    "\n",
    "    def evaluate_validation_set(self, timesteps):\n",
    "        latent = np.random.randn(self.x_val[timesteps][0].shape[0], timesteps, self.latent_dim)\n",
    "        y_pred = self.generator.predict([self.x_val[timesteps][0], self.x_val[timesteps][1], latent])\n",
    "        y_true = self.y_val[timesteps]\n",
    "        cat_acc = self.three_d_categorical_accuracy(y_true, y_pred)\n",
    "        mae = abs(y_pred[1] - y_true[1]).mean()\n",
    "        print('\\n\\n*******************')\n",
    "        print('\\n\\nCategorical accuracy:', cat_acc)\n",
    "        print('Mean absolute error:', mae)\n",
    "        print('\\n\\n')\n",
    "\n",
    "        \n",
    "    def add_group_shift(self, row):\n",
    "        if ((row['equivalency_flag'] & row['equivalency_shift'] | ~row['equivalency_flag'] & row['equivalency_shift']) | row['case_shift']):\n",
    "            self.group += 1\n",
    "        return self.group\n",
    "\n",
    "    \n",
    "    def repair_timestamps(self):\n",
    "            \"\"\"\n",
    "            Estimates the time differences of affected events on which basis the corrupted timestamp can be repaired\n",
    "\n",
    "            Parameters:\n",
    "                output_path (str): root path to where cGAN files were saved\n",
    "                reordered_log (pandas.DataFrame): the reordered event log\n",
    "                n_fallback_preds (int): the number of predictions to make in case of fallback\n",
    "\n",
    "            Returns:\n",
    "                repaired_log (pandas.DataFrame): The reordered and reestimated event log.\n",
    "            \"\"\"\n",
    "\n",
    "            model = self.generator\n",
    "            reordered_log = self.reordered_log.copy().sort_values(by=['id', 'timestamp'])\n",
    "            x = reordered_log.copy()\n",
    "            # indicate change of equivalency_flag\n",
    "            x['equivalency_shift'] = x['equivalency_flag'].shift() != x['equivalency_flag']\n",
    "            x.loc[0, 'equivalency_shift'] = x.loc[0, 'equivalency_flag']\n",
    "            # indicate change of case id\n",
    "            x['case_shift'] = x['id'].shift() != x['id']\n",
    "            x.loc[0, 'case_shift'] = False\n",
    "            # add sequence group id\n",
    "            x['group_id'] = x.apply(self.add_group_shift, axis=1)\n",
    "            to_be_repaired = x[x.equivalency_flag]\n",
    "            \n",
    "            median_case_duration = self.df.groupby('id').duration.sum().median()\n",
    "\n",
    "            print(\"Estimating duration for erroneous events...\")\n",
    "\n",
    "\n",
    "            reordered_log['predicted_timestamp'] = reordered_log['timestamp']\n",
    "            \n",
    "            for group_id in tqdm(to_be_repaired.group_id.unique()):\n",
    "                current = to_be_repaired[to_be_repaired['group_id'] == group_id]\n",
    "                input_activities = self.preprocessor.encoder.transform(current[['activity']]).reshape(1, current.shape[0], -1)\n",
    "                valid_subsequent_events = self.df[(self.df['timestamp'] > current.timestamp.iloc[0]) & (self.df['id'] == current.id.iloc[0])]\n",
    "                if valid_subsequent_events.shape[0] == 0:\n",
    "                    max_duration = median_case_duration\n",
    "                else:\n",
    "                    max_duration = (valid_subsequent_events.timestamp.min() - current.timestamp.iloc[0]).total_seconds() / current.shape[0]\n",
    "                max_duration_scaled = self.preprocessor.scaler.transform([[max_duration]])\n",
    "                max_duration_scaled = np.resize(max_duration_scaled, (current.shape[0], 1))\n",
    "                max_duration_scaled = max_duration_scaled.reshape(1, current.shape[0], -1)\n",
    "                latent = np.random.randn(1, current.shape[0], 100)\n",
    "                pred_durations = self.generator.predict([input_activities, max_duration_scaled, latent],verbose=0)[1]\n",
    "                pred_seconds = self.preprocessor.scaler.inverse_transform(pred_durations.reshape(1,-1)).reshape(-1)\n",
    "                current.predicted_timestamp = current.timestamp + pd.to_timedelta(pred_seconds, unit='s')\n",
    "                repair.reordered_log.update(current.iloc[:, 0:-3])\n",
    "            \n",
    "            self.repaired_log = reordered_log\n",
    "            self.reordered_log.to_csv(self.output_path + 'repaired_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2916f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "repair = ConfortiGAN('../../Daten/real_dataset/real_data_10.csv', 'id', 'timestamp', 'activity', 'output/')\n",
    "repair.prepare_data()\n",
    "repair.reorder_events()\n",
    "repair.construct_networks()\n",
    "repair.train_gan()\n",
    "repair.repair_timestamps()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
