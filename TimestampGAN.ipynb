{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from itertools import permutations\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Reshape, LSTM, LeakyReLU, GaussianNoise\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "class TimestampGAN:\n",
    "    \"\"\"\n",
    "    Class for repairing identical timestamp errors in event logs.\n",
    "    Instructions:\n",
    "        1.) Initialize object\n",
    "        2.) object.read_data()\n",
    "        3.) object.preprocess_data()\n",
    "        4.) object.construct_networks()\n",
    "        5.) object.train()\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        self.df_original = None\n",
    "        self.n_suffix = None\n",
    "        self.scaler = None\n",
    "        self.encoder = None\n",
    "        self.current_activities = None\n",
    "        self.current_deltas = None\n",
    "        self.succeeding_activities = None\n",
    "        self.succeeding_deltas = None\n",
    "        self.latent_dim = None\n",
    "        self.generator = None\n",
    "        self.discriminator = None\n",
    "        self.gan = None\n",
    "        self.is_skewed = False\n",
    "        \n",
    "        \n",
    "    def read_data(self, data_path, id_column, timestamp_column, activity_column):\n",
    "        \"\"\"\n",
    "        Reads the given data as a pandas dataframe and drops irrelevant columns.\n",
    "        \n",
    "        Parameters:\n",
    "            data_path (str): The path where the .csv or .xes file is stored (e.g. 'C:/event_log.csv')\n",
    "            id_column (str): The name of the column, that contains the case id. With .xes typically 'case:concept:name'\n",
    "            timestamp_column (str): The name of the column that contains the timestamp. With .xes typically 'time:timestamp'\n",
    "            activity_column (str): The name of the column that contains the activity. With .xes typically 'concept:name'\n",
    "        \"\"\"\n",
    "        if data_path.endswith('.csv'):\n",
    "            print('Reading .csv file...')\n",
    "            df = pd.read_csv(data_path)\n",
    "        else:\n",
    "            print('Reading .xes file...')\n",
    "            xes_file = pm4py.read_xes(data_path)\n",
    "            df = log_converter.apply(xes_file, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "        df = df[[id_column, timestamp_column, activity_column]]\n",
    "        df.columns = ['id', 'timestamp', 'activity']\n",
    "        df = df.sort_values(['id', 'timestamp']).reset_index(drop=True)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True) # ensure timestamp datetype\n",
    "        self.df = df\n",
    "        \n",
    "    def preprocess_data(self, n_suffix = 1):\n",
    "        \"\"\"\n",
    "        Applies timestamp transformation (time difference, log tranformation and min max scaling), \n",
    "        activity encoding (one hot encoding), as well as the construction of the final dataset.\n",
    "        \n",
    "        Parameters:\n",
    "            n_suffix (int): Number of succeeding events to take into account.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Detecting identical timestamp errors...\")\n",
    "        self.detect_error()\n",
    "        \n",
    "        print(\"Preprocessing data...\")\n",
    "        self.n_suffix = n_suffix\n",
    "        \n",
    "        # Add time difference\n",
    "        self.df['delta_t'] = self.df['timestamp'].diff().shift(0).dt.total_seconds()\n",
    "        self.df = self.df.dropna()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        \n",
    "        # replace the time difference of the first event of a case with the median difference of its activity\n",
    "        first_indices = self.df.reset_index().groupby('id').first()['index'].to_list()\n",
    "        not_first_indices = set(self.df.reset_index()['index'].to_list()) - set(first_indices)\n",
    "        not_first_indices = list(not_first_indices)\n",
    "        not_first_indices.sort()\n",
    "        delta_t_median = self.df.iloc[not_first_indices].groupby('activity')['delta_t'].median().to_dict()\n",
    "        replace_func = lambda x: delta_t_median[x['activity']] if x['activity'] in delta_t_median else 0\n",
    "        self.df.loc[first_indices, 'delta_t'] = self.df.iloc[first_indices].apply(replace_func, axis=1)\n",
    "        \n",
    "        # Replace duration of lines with identical timestamp errors with median of its activity\n",
    "        erroneous_rows = self.df.reset_index()[self.df['delta_t'] == 0]['index'].to_list()\n",
    "        self.df.loc[erroneous_rows, 'delta_t'] = self.df.iloc[erroneous_rows].apply(replace_func, axis=1)\n",
    "        \n",
    "        # Min max scale time difference\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.df['delta_t_norm'] = self.scaler.fit_transform(self.df[['delta_t']])\n",
    "        \n",
    "        # One hot encode activities\n",
    "        self.encoder = OneHotEncoder(sparse=False)\n",
    "        self.df['activities_encoded'] = self.encoder.fit_transform(self.df[['activity']]).tolist()\n",
    "        \n",
    "        # save the current df as df_original for later evaluation\n",
    "        self.df_original = self.df.copy()\n",
    "        self.df_original.loc[erroneous_rows, 'delta_t'] = 0\n",
    "        self.df_original['delta_t_norm'] = self.scaler.fit_transform(self.df_original[['delta_t']])\n",
    "        \n",
    "        \n",
    "        self.df = self.df.dropna().reset_index(drop=True)\n",
    "        \n",
    "        # drop cases with less than n_suffix events\n",
    "        drop_dict = (self.df.groupby('id').count()['timestamp'] <= self.n_suffix).to_dict() # identify case_ids with less than n_suffix activities\n",
    "        to_keep = [x for x in self.df['id'].unique() if not drop_dict[x]] # keep remaining rows\n",
    "        self.df = self.df[self.df['id'].isin(to_keep)].reset_index(drop=True)\n",
    "        \n",
    "        # Construct sequential dataset for the GAN\n",
    "        self.current_activities, self.current_deltas, self.succeeding_activities, self.succeeding_deltas = self.construct_sequential_dataset()\n",
    "        \n",
    "    def detect_error(self):\n",
    "        \"\"\"\n",
    "        Detects identical timestamp errors in a dataframe. Adds an equivalency_flag attribute to \n",
    "        the dataframe.\n",
    "        \n",
    "        Parameters:\n",
    "            \n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        booleans = [[] for x in range(len(df))]\n",
    "        booleans[0].append(False)\n",
    "        booleans[-1].append(False)\n",
    "        for i in tqdm(range(df.shape[0] - 1)):\n",
    "            # different case\n",
    "            if df.iloc[i, 0] != df.iloc[i+1, 0]:\n",
    "                booleans[i].append(False)\n",
    "                booleans[i+1].append(False)\n",
    "                continue\n",
    "            # same case\n",
    "            else:\n",
    "                # timestamp-equivalent events\n",
    "                if df.iloc[i, 1] == df.iloc[i+1, 1]:\n",
    "                    booleans[i].append(True)\n",
    "                    booleans[i+1].append(True)\n",
    "                # no timestamp-equivalent events\n",
    "                else:\n",
    "                    booleans[i].append(False)\n",
    "                    booleans[i+1].append(False)\n",
    "        equivalency_flag = [x[0] or x[1] for x in booleans]\n",
    "        self.df['equivalency_flag'] = equivalency_flag\n",
    "        \n",
    "        \n",
    "    def construct_sequential_dataset(self):\n",
    "        \"\"\"\n",
    "        Constructs the sequential dataset, which the GAN will take as input.\n",
    "        \n",
    "        Returns:\n",
    "            current_activity (np.array): The one hot encoded activities of the current activity\n",
    "            succeeding_activities (np.array): The one hot encoded activities of the suffix events\n",
    "            succeeding_deltas (np.array): The transformed suffix time difference\n",
    "            current_delta (np.array): The current transformed and scaled time difference\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Constructing sequential dataset...\")\n",
    "        current_activity = []\n",
    "        current_delta = []\n",
    "        succeeding_activities = []\n",
    "        succeeding_deltas = []\n",
    "        \n",
    "        \n",
    "        id_to_row_index = self.df.groupby('id').groups\n",
    "        unique_ids = self.df['id'].unique()\n",
    "        \n",
    "        for case_id in tqdm(unique_ids):\n",
    "            current_set = self.df.iloc[id_to_row_index[case_id]].reset_index()\n",
    "            for i in range(0, current_set.shape[0]):\n",
    "                if (i + self.n_suffix) >= current_set.shape[0]:\n",
    "                    continue\n",
    "                current_act = np.array([current_set.loc[i, 'activities_encoded']])\n",
    "                succeeding_act = np.array(current_set.loc[i+1:i+self.n_suffix, 'activities_encoded'].to_list())\n",
    "                \n",
    "                current_delt = np.array([current_set.loc[i, 'delta_t_norm']])\n",
    "                succeeding_delt = np.array(current_set.loc[i+1:i+self.n_suffix, 'delta_t_norm'].to_list())\n",
    "                \n",
    "                current_activity.append(current_act)\n",
    "                current_delta.append(current_delt)\n",
    "                succeeding_activities.append(succeeding_act)\n",
    "                succeeding_deltas.append(succeeding_delt)\n",
    "                \n",
    "        current_activity = np.array(current_activity)\n",
    "        current_delta = np.array(current_delta)\n",
    "        succeeding_activities = np.array(succeeding_activities)\n",
    "        succeeding_deltas = np.array(succeeding_deltas)\n",
    "        return current_activity, current_delta, succeeding_activities, succeeding_deltas\n",
    "    \n",
    "    def construct_networks(self, latent_dim = 100):\n",
    "        \"\"\"\n",
    "        Constructs the generator, discriminator and the GAN.\n",
    "        \n",
    "        Parameters:\n",
    "            latent_dim (int): The dimension of the latent space\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"Constructing networks...\")\n",
    "        self.latent_dim = latent_dim\n",
    "        self.generator = self.define_generator(n_classes = self.encoder.categories_[0].shape[0], latent_dim = latent_dim)\n",
    "        self.discriminator = self.define_discriminator(n_classes = self.encoder.categories_[0].shape[0])\n",
    "        self.gan = self.define_gan()\n",
    "        \n",
    "        \n",
    "    def define_generator(self, n_classes, latent_dim):\n",
    "        \"\"\"\n",
    "        Defines the generator.\n",
    "        \n",
    "        Parameters:\n",
    "            n_classes (int): The amount of unique activities performed in the event log\n",
    "            latent_dim (int): The dimension of the latent space\n",
    "            \n",
    "        Returns:\n",
    "            generator (keras.Functional): The generator network\n",
    "        \"\"\"\n",
    "        # one hot encoded activities as input\n",
    "        input_current_activity = Input(shape = (1, n_classes), name='input_current_activity_one_hot_encoded')\n",
    "        input_succeeding_activity = Input(shape = (self.n_suffix, n_classes), name='input_succeeding_activities_one_hot_encoded')\n",
    "        layer_activities = Concatenate()([input_current_activity, input_succeeding_activity])\n",
    "        layer_activities = Reshape((2, -1))(layer_activities)\n",
    "        \n",
    "        # time difference inputs\n",
    "        input_succeeding_deltas = Input(shape=(self.n_suffix), name='input_succeeding_deltas')\n",
    "        layer_deltas = Dense(2)(input_succeeding_deltas)\n",
    "        layer_deltas = Reshape((2,-1))(layer_deltas)\n",
    "        \n",
    "        # latent space input\n",
    "        input_latent = Input(shape=(latent_dim,), name='gaussian_input')\n",
    "        layer_latent = Dense(latent_dim)(input_latent)        \n",
    "        layer_latent = Reshape((2, -1))(layer_latent)\n",
    "        \n",
    "        \n",
    "        # combining the different inputs\n",
    "        layer = Concatenate()([layer_activities, layer_deltas, layer_latent])\n",
    "        \n",
    "        # hidden layers\n",
    "        layer = Reshape((2,-1))(layer)\n",
    "        layer = LSTM(500, return_sequences=True, dropout=0.25)(layer)\n",
    "        layer = LSTM(300, return_sequences=False, activation='relu')(layer)\n",
    "        layer = Dense(100)(layer)\n",
    "        layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        layer = Dense(10)(layer)\n",
    "        layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        layer = Dense(1, activation='sigmoid')(layer)\n",
    "        generator = Model([input_current_activity, input_succeeding_activity, \n",
    "                           input_succeeding_deltas, input_latent], layer)\n",
    "        return generator\n",
    "    \n",
    "    def define_discriminator(self, n_classes):\n",
    "        \"\"\"\n",
    "        Defines the discriminator.\n",
    "        \n",
    "        Parameters:\n",
    "            n_classes (int): The amount of unique activities performed in the event log\n",
    "            \n",
    "        Returns:\n",
    "            discriminator (keras.Functional): The discriminator network\n",
    "        \"\"\"\n",
    "        # one hot encoded activities as input\n",
    "        input_current_activity = Input(shape = (1, n_classes), name='input_current_activity_one_hot_encoded')\n",
    "        input_succeeding_activity = Input(shape = (self.n_suffix, n_classes), name='input_succeeding_activities_one_hot_encoded')\n",
    "        layer_activities = Concatenate()([input_current_activity, input_succeeding_activity])\n",
    "        layer_activities = Reshape((2, -1))(layer_activities)\n",
    "        \n",
    "        \n",
    "        # timestamp inputs\n",
    "        input_current_deltas = Input(shape=(1), name='input_current_deltas')\n",
    "        input_succeeding_deltas = Input(shape=(self.n_suffix), name='input_succeeding_deltas')\n",
    "        layer_deltas = Concatenate()([input_current_deltas, input_succeeding_deltas])\n",
    "        layer_deltas = Reshape((2,-1))(layer_deltas)\n",
    "        \n",
    "        # combining the different inputs\n",
    "        layer = Concatenate()([layer_activities, layer_deltas])\n",
    "        layer = GaussianNoise(0.1)(layer)\n",
    "        \n",
    "        # hidden layers\n",
    "        layer = Reshape((2,-1))(layer)\n",
    "        layer = LSTM(500, return_sequences=True)(layer)\n",
    "        layer = LSTM(300, activation='relu', return_sequences=False)(layer)\n",
    "        layer = Dense(50)(layer)\n",
    "        layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        layer = Dense(10)(layer)\n",
    "        layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        layer = Dense(1, activation='sigmoid')(layer)\n",
    "        \n",
    "        discriminator = Model([input_current_activity, input_succeeding_activity,\n",
    "                               input_current_deltas, input_succeeding_deltas], layer)\n",
    "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "        discriminator.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return discriminator\n",
    "        \n",
    "    def define_gan(self):\n",
    "        \"\"\"\n",
    "        Constructs the GAN by combining the discriminator and the generator.\n",
    "        \n",
    "        Returns:\n",
    "            gan (keras.Functional): The complete GAN\n",
    "        \"\"\"\n",
    "        self.discriminator.trainable=False\n",
    "        input_current_activity, input_succeeding_activity, input_succeeding_deltas, input_latent = self.generator.input\n",
    "        generator_output = self.generator.output\n",
    "        gan_output = self.discriminator([input_current_activity, input_succeeding_activity, generator_output, input_succeeding_deltas])\n",
    "        gan = Model([input_current_activity, input_succeeding_activity, input_succeeding_deltas, input_latent], gan_output)\n",
    "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "        gan.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "        return gan\n",
    "    \n",
    "    def get_real_samples(self, n_samples):\n",
    "        \"\"\"\n",
    "        Randomly picks n real samples of data.\n",
    "        \n",
    "        Parameters:\n",
    "            n_samples (int): The amount of samples to be picked.\n",
    "            \n",
    "        Returns:\n",
    "            current_act (np.array): The current activity as a one hot encoded vector\n",
    "            succeeding_act (np.array): The suffix activities as one hot encoded vectors\n",
    "            current_time (np.array): The actual time difference of the current event\n",
    "            succeeding_time (np.array): The suffix time differences\n",
    "            y (np.array): The labels of the samples, in this case 1.\n",
    "        \"\"\"\n",
    "        indices = np.random.randint(0, self.current_activities.shape[0], n_samples)\n",
    "        \n",
    "        # activities\n",
    "        current_act = self.current_activities[indices]\n",
    "        succeeding_act = self.succeeding_activities[indices]\n",
    "        \n",
    "        # timestamps\n",
    "        current_time = self.current_deltas[indices]\n",
    "        succeeding_time = self.succeeding_deltas[indices]\n",
    "        \n",
    "        # labels\n",
    "        y = np.ones((n_samples, 1))\n",
    "        return [current_act, succeeding_act, current_time, succeeding_time], y\n",
    "    \n",
    "    def get_generator_input(self, n_samples):\n",
    "        \"\"\"\n",
    "        Constructs the generator's input by randomly picking samples of data as well as latent input.\n",
    "        \n",
    "        Parameters:\n",
    "            n_samples (int): The number of samples to be picked.\n",
    "            \n",
    "        Returns:\n",
    "            current_act (np.array): The current activity as a one hot encoded vector\n",
    "            succeeding_act (np.array): The suffix activities as one hot encoded vectors\n",
    "            succeeding_time (np.array): The suffix time differences\n",
    "            latent (np.array): The random gaussian noise\n",
    "        \"\"\"\n",
    "        # indices\n",
    "        indices = np.random.randint(0, self.current_activities.shape[0], n_samples)\n",
    "        \n",
    "        # activities\n",
    "        current_act = self.current_activities[indices]\n",
    "        succeeding_act = self.succeeding_activities[indices]\n",
    "        \n",
    "        # timestamps\n",
    "        succeeding_time = self.succeeding_deltas[indices]\n",
    "        \n",
    "        # gaussian noise\n",
    "        latent = np.random.randn(n_samples, self.latent_dim)\n",
    "        return [current_act, succeeding_act, succeeding_time, latent]\n",
    "        \n",
    "    def generate_fake_samples(self, n_samples):\n",
    "        \"\"\"\n",
    "        Generates n_samples of data by the generator.\n",
    "        \n",
    "        Parameters:\n",
    "            n_samples (int): The amount of samples to be generated.\n",
    "            \n",
    "        Returns:\n",
    "            current_act (np.array): The current activity as a one hot encoded vector\n",
    "            succeeding_act (np.array): The suffix activities as one hot encoded vectors\n",
    "            generated (np.array): The generated time difference for the current event\n",
    "            succeeding_time (np.array): The suffix time differences\n",
    "            y (np.array): The labels of the samples, in this case 0.\n",
    "        \"\"\"\n",
    "        current_act, succeeding_act, succeeding_time, latent = self.get_generator_input(n_samples)\n",
    "        generated = self.generator.predict([current_act, succeeding_act, succeeding_time, latent])\n",
    "        y = np.zeros((n_samples,1))\n",
    "        return [current_act, succeeding_act, generated, succeeding_time], y\n",
    "\n",
    "    \n",
    "    def train(self, n_epochs, n_batch, output_path):\n",
    "        \"\"\"\n",
    "        Trains the GAN.\n",
    "        \n",
    "        Parameters:\n",
    "            n_epochs (int): The amount of epochs, the GAN should be trained for.\n",
    "            n_batch (int): The batch size for a single training run.\n",
    "            output_path (str): The output path, where model files as well as logging will be saved.\n",
    "            \n",
    "        Returns:\n",
    "            d_loss_real (list): The historical accuracies of the discriminator on real samples\n",
    "            d_loss_fake (list): The historical accuracies of the discriminator on fake samples\n",
    "            g_loss (list): The historical accuracies of the generator\n",
    "        \"\"\"\n",
    "        print(\"Creating folder structure...\")\n",
    "        os.mkdir(output_path)\n",
    "        os.mkdir(output_path + '/model_files')\n",
    "        os.mkdir(output_path + '/logging')\n",
    "        \n",
    "        print(\"Training the networks...\")\n",
    "        bat_per_epo = int(self.current_activities.shape[0] / n_batch)\n",
    "        half_batch = int(n_batch / 2)\n",
    "        \n",
    "        d_loss_real = []\n",
    "        d_loss_fake = []\n",
    "        g_loss = []\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            for j in range(bat_per_epo):\n",
    "            \n",
    "                # get randomly selected 'real' samples\n",
    "                disc_input_real, y_real = self.get_real_samples(half_batch)\n",
    "                d_loss1, _ = self.discriminator.train_on_batch(disc_input_real, y_real, verbose=0)\n",
    "                d_loss_real.append(d_loss1)\n",
    "                \n",
    "                # get randomly selected 'fake' samples\n",
    "                disc_input_fake, y_fake = self.generate_fake_samples(half_batch)\n",
    "                d_loss2, _ = self.discriminator.train_on_batch(disc_input_fake, y_fake, verbose=0)\n",
    "                d_loss_fake.append(d_loss2)\n",
    "                \n",
    "                x_gan = self.get_generator_input(n_batch)\n",
    "                y_gan = np.ones((n_batch, 1))\n",
    "                gan_loss = self.gan.train_on_batch(x_gan, y_gan, verbose=0)\n",
    "                g_loss.append(gan_loss)\n",
    "                \n",
    "                print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %(i+1, j+1, bat_per_epo, d_loss1, d_loss2, gan_loss))\n",
    "                \n",
    "                file = open(output_path + '/logging/metric_log.txt', 'a')\n",
    "                file.write('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %(i+1, j+1, bat_per_epo, d_loss1, d_loss2, gan_loss))\n",
    "                file.write('\\n')\n",
    "                file.close()\n",
    "                \n",
    "                if j % 250 == 0:    \n",
    "                    self.generator.save(output_path + '/model_files/epoch_' + str(i).zfill(4) + '_batch_' + str(j).zfill(4) + '.h5')\n",
    "                    eval_pred = self.evaluate_random_sample(25000)\n",
    "                    print('MAE of random evaluation:', eval_pred)\n",
    "                    file = open(output_path + '/logging/eval_log.txt', 'a')\n",
    "                    file.write('>%d, %d/%d, d1=%.3f' %(i+1, j+1, bat_per_epo, eval_pred))\n",
    "                    file.write('\\n')\n",
    "                    file.close()\n",
    "        return d_loss_real, d_loss_fake, g_loss\n",
    "        \n",
    "        \n",
    "    def evaluate_random_sample(self, n_samples):\n",
    "        \"\"\"\n",
    "        Generates a prediction for n random samples and compares it to its actual value.\n",
    "        \n",
    "        Parameters:\n",
    "            n_samples (int): The amount of evaluation samples that should be considered.\n",
    "        \n",
    "        Returns:\n",
    "            difference (float): The mean absolute error\n",
    "        \"\"\"\n",
    "        # indices\n",
    "        indices = np.random.randint(0, self.current_activities.shape[0], n_samples)\n",
    "        \n",
    "        # activities\n",
    "        current_act = self.current_activities[indices]\n",
    "        succeeding_act = self.succeeding_activities[indices]\n",
    "        \n",
    "        # timestamps\n",
    "        succeeding_time = self.succeeding_deltas[indices]\n",
    "        \n",
    "        # gaussian noise\n",
    "        latent = np.random.randn(n_samples, self.latent_dim)\n",
    "        \n",
    "        # compare predicted to actual timestamp\n",
    "        actual = self.current_deltas[indices]\n",
    "        preds = self.generator.predict([current_act, succeeding_act, succeeding_time, latent])\n",
    "        difference = abs(actual - preds).mean()\n",
    "        return difference\n",
    "        \n",
    "    def repair_event_log(self, output_path):\n",
    "        \"\"\"\n",
    "        Repairs an event log affected by identical timestamp errors.\n",
    "        \n",
    "        Parameters:\n",
    "            output_path (str): root path to where cGAN files were saved\n",
    "        \n",
    "        Returns:\n",
    "            repaired_log (pandas.DataFrame): the repaired event log\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"Reordering erroneous events...\")\n",
    "        #reordered_log = self.reorder_events()\n",
    "        \n",
    "        print(\"Repairing corrupted timestamps...\")\n",
    "        #repaired_log = self.repair_timestamps(output_path, reordered_log)\n",
    "        repaired_log = self.repair_timestamps(output_path, self.df_original)\n",
    "        return repaired_log\n",
    "\n",
    "    def reorder_events(self):\n",
    "        \"\"\"\n",
    "        Reorders events affected by identical timestamp errors.\n",
    "        \n",
    "        Parameters:\n",
    "        \n",
    "        Returns:\n",
    "            reordered_log (pandas.DataFrame): The reordered event log\n",
    "        \"\"\"\n",
    "        indices = self.df_original[self.df_original.equivalency_flag].activity.reset_index()['index'].to_list()\n",
    "        old_order = self.df_original[self.df_original.equivalency_flag].activity.to_list()\n",
    "        \n",
    "        best_order = self.get_best_order()\n",
    "        best_order = best_order.best_order.to_list()\n",
    "        flat_list = [item for sublist in best_order for item in sublist]\n",
    "        best_order = flat_list\n",
    "        \n",
    "        old_order_indexed = []\n",
    "        index_dict = {key : 0 for key in set(old_order)}\n",
    "        for el in old_order:\n",
    "            old_order_indexed.append(el + str(index_dict[el]))\n",
    "            index_dict[el] += 1\n",
    "        \n",
    "        best_order_indexed = []\n",
    "        index_dict = {key : 0 for key in set(best_order)}\n",
    "        for el in best_order:\n",
    "            best_order_indexed.append(el + str(index_dict[el]))\n",
    "            index_dict[el] += 1\n",
    "        \n",
    "        res = sorted(old_order_indexed, key = best_order_indexed.index)\n",
    "        sorting = [old_order_indexed.index(x) for x in res]\n",
    "        new_order = [indices[i] for i in sorting]\n",
    "        for i in range(self.df_original.shape[0]):\n",
    "            if i in new_order:\n",
    "                continue\n",
    "            else:\n",
    "                new_order.insert(i, i)\n",
    "        reordered_log = self.df_original.iloc[new_order].reset_index(drop=True)\n",
    "        return reordered_log\n",
    "    \n",
    "    def get_pairwise_confidences(self):\n",
    "        \"\"\"\n",
    "        Calculates the pairwise confidences for all activity combinations in the event log.\n",
    "        \n",
    "        Parameters:\n",
    "        \n",
    "        Returns:\n",
    "            pairwise_confidences (dict): Mapping of activity combination to confidence\n",
    "        \"\"\"\n",
    "        temp = self.df_original[~self.df_original['equivalency_flag']]\n",
    "        temp = temp.reset_index(drop=True)\n",
    "        occurences = {}\n",
    "        last = temp.reset_index().groupby('id').last()['index'].to_list()\n",
    "\n",
    "\n",
    "        for activity in tqdm(temp.activity.unique()):\n",
    "            subs = list(temp[temp['activity'] == activity].index + 1)\n",
    "            subs = [x for x in subs if x not in last and x < temp.shape[0]]\n",
    "            occurences[activity] = temp.iloc[subs,2].value_counts().to_dict()\n",
    "\n",
    "        pairwise_confidences = {}\n",
    "\n",
    "        for activity in temp.activity.unique():\n",
    "            for other_activity in temp.activity.unique():\n",
    "                if other_activity not in occurences[activity].keys():\n",
    "                    pairwise_confidences[(activity, other_activity)] = 0\n",
    "                else:\n",
    "                    conf = occurences[activity][other_activity] / temp[temp['activity'] == activity].shape[0]\n",
    "                    pairwise_confidences[(activity, other_activity)] = conf\n",
    "        return pairwise_confidences\n",
    "    \n",
    "    def get_best_order(self):\n",
    "        \"\"\"\n",
    "        Calculates the best order for affected events.\n",
    "        \n",
    "        Returns:\n",
    "            best_order (pandas.DataFrame): The best orders for timestamp-equivalent sequence in form of a list\n",
    "        \"\"\"\n",
    "        pairwise_confidences = self.get_pairwise_confidences()\n",
    "        to_repair = self.df_original[self.df_original['equivalency_flag']].groupby(['id', 'timestamp'])['activity'].apply(list).reset_index()\n",
    "        best_sequences = []\n",
    "        \n",
    "        for i in tqdm(range(to_repair.shape[0])):\n",
    "            sequence = to_repair.iloc[i, -1]\n",
    "            if len(sequence) > 9:\n",
    "                best_sequences.append(sequence)\n",
    "                continue\n",
    "            perms = list(permutations(sequence))\n",
    "            sequence_confidences = []\n",
    "\n",
    "            for perm in perms:\n",
    "                seq = [(perm[i], perm[i+1]) for i in range(len(perm) - 1)]\n",
    "                try:\n",
    "                    seq_conf = sum([pairwise_confidences[x] for x in seq])\n",
    "                except:\n",
    "                    seq_conf = 0\n",
    "                sequence_confidences.append(seq_conf)\n",
    "\n",
    "            best_sequence = perms[sequence_confidences.index(max(sequence_confidences))]\n",
    "            best_sequences.append(list(best_sequence))\n",
    "\n",
    "        to_repair['best_order'] = best_sequences\n",
    "        return to_repair\n",
    "    \n",
    "    def repair_timestamps(self, output_path, reordered_log, n_fallback_preds = 200):\n",
    "        \"\"\"\n",
    "        Estimates the time differences of affected events on which basis the corrupted timestamp can be repaired\n",
    "        \n",
    "        Parameters:\n",
    "            output_path (str): root path to where cGAN files were saved\n",
    "            reordered_log (pandas.DataFrame): the reordered event log\n",
    "            n_fallback_preds (int): the number of predictions to make in case of fallback\n",
    "            \n",
    "        Returns:\n",
    "            repaired_log (pandas.DataFrame): The reordered and reestimated event log.\n",
    "        \"\"\"\n",
    "        \n",
    "        model = self.get_best_model(output_path)\n",
    "        \n",
    "        print(\"Estimating time difference for erroneous events...\")\n",
    "        \n",
    "        # recalculate delta_t_norm since order of events has changed\n",
    "        reordered_log['delta_t'] = reordered_log['timestamp'].diff().shift(0).dt.total_seconds()\n",
    "        reordered_log = reordered_log.dropna()\n",
    "        reordered_log = reordered_log.reset_index(drop=True)\n",
    "        \n",
    "        # replace the time difference of the first event of a case with the median difference of its activity\n",
    "        first_indices = reordered_log.reset_index().groupby('id').first()['index'].to_list()\n",
    "        not_first_indices = set(reordered_log.reset_index()['index'].to_list()) - set(first_indices)\n",
    "        not_first_indices = list(not_first_indices)\n",
    "        not_first_indices.sort()\n",
    "        delta_t_median = reordered_log.iloc[not_first_indices].groupby('activity')['delta_t'].median().to_dict()\n",
    "        replace_func = lambda x: delta_t_median[x['activity']] if x['activity'] in delta_t_median else 0\n",
    "        reordered_log.loc[first_indices, 'delta_t'] = reordered_log.iloc[first_indices].apply(replace_func, axis=1)\n",
    "        \n",
    "        reordered_log['delta_t_norm'] = self.scaler.fit_transform(reordered_log[['delta_t']])\n",
    "        \n",
    "        reordered_log['predicted_delta'] = reordered_log['delta_t']\n",
    "        \n",
    "        success_count = 0\n",
    "        fallback_count = 0\n",
    "        change_log = []\n",
    "        \n",
    "        try:\n",
    "            for i in range(0, reordered_log.shape[0] - 1):\n",
    "                current = reordered_log.iloc[i]\n",
    "                # Correct event present, no need to repair\n",
    "                if (current['delta_t'] != 0):\n",
    "                    change_log.append('No change')\n",
    "                    continue\n",
    "                else:\n",
    "                    # Get next correct delta t\n",
    "                    succeeding_count = 1\n",
    "                    while True:\n",
    "                        succeeding = reordered_log.iloc[i + succeeding_count : i + succeeding_count + self.n_suffix]\n",
    "                        if succeeding.iloc[0]['delta_t'] == current['delta_t']:\n",
    "                            succeeding_count += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    # Construct generator inputs\n",
    "                    current_activity = np.array([current['activities_encoded']]).reshape(1,1,-1)\n",
    "                    current_activity_repeat = np.tile(current_activity,[n_fallback_preds,1,1])\n",
    "                    succeeding_activity = np.array(succeeding['activities_encoded'].to_list()).reshape(1,repair.n_suffix,-1)\n",
    "                    succeeding_activity_repeat = np.tile(succeeding_activity,[n_fallback_preds,1,1])\n",
    "                    succeeding_deltas = np.array(succeeding['delta_t_norm'].to_list()).reshape(1,repair.n_suffix)\n",
    "                    succeeding_deltas_repeat = np.tile(succeeding_deltas, [n_fallback_preds,1])\n",
    "                    gauss_repeat = np.random.randn(n_fallback_preds, repair.latent_dim)\n",
    "\n",
    "                    # Estimate time difference\n",
    "                    preds = model.predict([current_activity_repeat, succeeding_activity_repeat,\n",
    "                                          succeeding_deltas_repeat, gauss_repeat])\n",
    "\n",
    "                    # Reverse transform the predicted time difference\n",
    "                    preds = self.scaler.inverse_transform(preds)\n",
    "\n",
    "                    # Check if prediction is valid or if fallback needs to be applied\n",
    "                    valid_indices = np.where((preds < succeeding.iloc[-1]['delta_t']) & (preds >= 0))[0]\n",
    "                    if valid_indices.shape[0] > 0:\n",
    "                        new_delta = int(preds[valid_indices[0]])\n",
    "                        success_count += 1\n",
    "                        change_log.append('Single pred')\n",
    "                    elif int(preds.mean()) < succeeding.iloc[-1]['delta_t']:\n",
    "                        new_delta = preds.mean()\n",
    "                        success_count += 1\n",
    "                        change_log.append('Mean pred')\n",
    "                    else:\n",
    "                        new_delta = 1\n",
    "                        fallback_count += 1\n",
    "                        change_log.append('Fallback')\n",
    "                        \n",
    "                    try:\n",
    "                        reordered_log.loc[i, 'predicted_delta'] = new_delta\n",
    "                    except Exception as e:\n",
    "                        print(e, 'at index', i)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        print('Repair successfull! Statistics:\\nSuccesful changes:', success_count, '\\nFallback changes:', fallback_count)\n",
    "        return reordered_log\n",
    "        \n",
    "    def get_best_model(self, output_path):\n",
    "        \"\"\"\n",
    "        Determines the best cGAN according to three consecutive evaluation losses.\n",
    "        \n",
    "        Parameters:\n",
    "            output_path (str): root path to where cGAN files were saved\n",
    "        \n",
    "        Returns:\n",
    "            best_model (keras.Functional): The best cGAN model\n",
    "        \"\"\"\n",
    "        print(\"Determining best cGAN model...\")\n",
    "        f = open(output_path + '/logging/eval_log.txt')\n",
    "        line = f.readline()\n",
    "        eval_losses = []\n",
    "        \n",
    "        while line:\n",
    "            loss = line.split('d1=')[1].replace('\\n', '')\n",
    "            eval_losses.append(float(loss))\n",
    "            line = f.readline()\n",
    "\n",
    "        means = []\n",
    "        for i in range(3, len(eval_losses)):\n",
    "            means.append(sum(eval_losses[i-3 : i]) / 3)\n",
    "\n",
    "        best_model = means.index(min(means))\n",
    "        MODEL_PATH = output_path + '/model_files/'\n",
    "        MODEL_NAME = os.listdir(MODEL_PATH)[best_model]\n",
    "        best_model = keras.models.load_model(MODEL_PATH + MODEL_NAME)\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repair = TimestampGAN()\n",
    "repair.read_data('log.csv', 'id', 'timestamp', 'activity')\n",
    "repair.preprocess_data()\n",
    "repair.construct_networks()\n",
    "d_loss_real, d_loss_fake, g_loss = repair.train(n_epochs = 5, n_batch = 256, output_path = 'Training/test_output_40/')\n",
    "repaired_log = repair.repair_event_log(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
